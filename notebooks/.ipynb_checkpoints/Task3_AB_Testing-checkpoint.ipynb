{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5946776d",
   "metadata": {},
   "source": [
    "# Task 3: A/B Hypothesis Testing - Insurance Risk Analytics\n",
    "\n",
    "**Prepared by:** Bereket Feleke  \n",
    "**Program:** 10 Academy – AI Mastery, Week 3  \n",
    "**Date:** December 2025\n",
    "\n",
    "---\n",
    "\n",
    "**Objective:** Validate hypotheses about Claim Frequency, Claim Severity, and Margin across segments (Gender, Province, Branch, PostalCode, Vehicle Make/Model, TransactionMonth) using the company insurance dataset. This notebook includes inline interpretations and recommended business actions so it’s submission-ready.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542315f2",
   "metadata": {},
   "source": [
    "## Instructions & Configuration\n",
    "\n",
    "- Place `insurance.csv` in the same folder as this notebook or update `DATA_PATH` in the first code cell.\n",
    "- This notebook will create `AB_Test_Significant_Findings.csv` if any significant tests are found.\n",
    "- All hypothesis interpretations are inline under each step (as requested).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f5bf6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "data/raw/insurance.csv not found. Upload insurance.csv or update DATA_PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m TC = \u001b[33m'\u001b[39m\u001b[33mTotalClaims\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(DATA_PATH):\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found. Upload insurance.csv or update DATA_PATH\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m df = pd.read_csv(DATA_PATH)\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mLoaded:\u001b[39m\u001b[33m'\u001b[39m, DATA_PATH)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: data/raw/insurance.csv not found. Upload insurance.csv or update DATA_PATH"
     ]
    }
   ],
   "source": [
    "# Step 1 — Import libraries & load dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "DATA_PATH = 'data/raw/insurance.csv' # update if file is elsewhere\n",
    "TP = 'TotalPremium'\n",
    "TC = 'TotalClaims'\n",
    "\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"{DATA_PATH} not found. Upload insurance.csv or update DATA_PATH\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print('Loaded:', DATA_PATH)\n",
    "print('Rows, cols:', df.shape)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223329ed",
   "metadata": {},
   "source": [
    "## Step 2 — Data validation & quick overview\n",
    "\n",
    "**What we check here:**\n",
    "- Presence and types of expected columns (TotalPremium, TotalClaims, Province, PostalCode, Branch, Gender, Make, Model, TransactionMonth).\n",
    "- Missingness and duplicates.\n",
    "- Basic plausibility of monetary fields (no unexpected negative values, consistent units).\n",
    "\n",
    "**Why this matters (interpretation):**\n",
    "Bad or inconsistent data leads to incorrect test results (false positives/negatives). If `TotalPremium` or `TotalClaims` contain errors, loss-ratio calculations and all downstream tests will be misleading. Address missingness (>20-40% in any key column) before trusting results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec06d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data validation checks\n",
    "print('Shape:', df.shape)\n",
    "print('\\nColumn dtypes:')\n",
    "display(df.dtypes)\n",
    "\n",
    "print('\\nMissingness (top 50):')\n",
    "missing = df.isna().sum().rename('count').to_frame()\n",
    "missing['pct'] = (missing['count'] / len(df) * 100).round(3)\n",
    "display(missing.sort_values('pct', ascending=False).head(50))\n",
    "\n",
    "print('\\nDuplicate rows:', df.duplicated().sum())\n",
    "\n",
    "for col in [TP, TC, 'CalculatedPremiumPerTerm', 'CustomValueEstimate']:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\n{col}: min={df[col].min()}, max={df[col].max()}, mean={df[col].mean():.2f}, nulls={df[col].isna().sum()}\")\n",
    "\n",
    "# Clean common string columns\n",
    "for col in ['PostalCode','Province','Branch','Gender','Make','Model']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str).str.strip().replace({'nan': pd.NA, 'None': pd.NA})\n",
    "\n",
    "# Parse TransactionMonth if present\n",
    "if 'TransactionMonth' in df.columns:\n",
    "    try:\n",
    "        df['TransactionMonth'] = pd.to_datetime(df['TransactionMonth'])\n",
    "        print('\\nTransactionMonth parsed:', df['TransactionMonth'].min(), 'to', df['TransactionMonth'].max())\n",
    "    except Exception as e:\n",
    "        print('TransactionMonth parse error:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c554175",
   "metadata": {},
   "source": [
    "## Step 3 — Create core metrics (ClaimFrequency, ClaimSeverity, Margin, LossRatio)\n",
    "\n",
    "**Calculations:**\n",
    "- ClaimFrequency = 1 if TotalClaims > 0 else 0\n",
    "- ClaimSeverity = TotalClaims when a claim exists (NaN otherwise)\n",
    "- Margin = TotalPremium − TotalClaims\n",
    "- LossRatio = TotalClaims / TotalPremium (NaN when TotalPremium == 0)\n",
    "\n",
    "**Interpretation guidance:**\n",
    "- Portfolio-level Loss Ratio indicates overall underwriting adequacy. Compare to business target (e.g., 60%–80% depending on product).\n",
    "- High Claim Frequency with low Severity suggests many small claims — marketing or friction issues. High Severity with low Frequency suggests occasional catastrophes — reinsurance or reserves considerations.\n",
    "These metrics are the primary KPIs used in hypothesis testing and business decisions below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933fd1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create core metrics\n",
    "df['ClaimFrequency'] = np.where(df[TC] > 0, 1, 0)\n",
    "df['ClaimSeverity'] = df[TC].where(df[TC] > 0, np.nan)\n",
    "df['Margin'] = df[TP] - df[TC]\n",
    "df['LossRatio'] = np.where(df[TP] > 0, df[TC] / df[TP], np.nan)\n",
    "\n",
    "portfolio_loss_ratio = df[TC].sum() / df[TP].sum() if df[TP].sum() > 0 else np.nan\n",
    "print(f'Portfolio loss ratio: {portfolio_loss_ratio:.4f}')\n",
    "print(f'Average claim frequency: {df[\"ClaimFrequency\"].mean():.4%}')\n",
    "print(f'Average claim severity (conditional): {df[\"ClaimSeverity\"].mean(skipna=True):.2f}')\n",
    "\n",
    "display(df[[TP, TC, 'ClaimFrequency','ClaimSeverity','Margin','LossRatio']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa19c8f",
   "metadata": {},
   "source": [
    "## Step 4 — Univariate distribution & outlier diagnostics\n",
    "\n",
    "**What we do:** Plot histograms and boxplots for TotalPremium, TotalClaims, Margin, LossRatio and show robust summaries.\n",
    "\n",
    "**Why:** Insurance financials are typically heavy-tailed (a few large claims). Tests that assume normality (t-test) may be invalid for small samples — we use non-parametric alternatives when appropriate and consider log/winsor transforms for modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a25c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_plot = [TP, TC, 'Margin', 'LossRatio']\n",
    "present = [v for v in vars_to_plot if v in df.columns]\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(14, 3*len(present)))\n",
    "for i, v in enumerate(present, start=1):\n",
    "    plt.subplot(len(present), 2, 2*i-1)\n",
    "    sns.histplot(df[v].dropna(), kde=True)\n",
    "    plt.title(f'Histogram: {v}')\n",
    "    plt.subplot(len(present), 2, 2*i)\n",
    "    sns.boxplot(x=df[v].dropna())\n",
    "    plt.title(f'Boxplot: {v}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "summary = df[present].describe(percentiles=[0.01,0.05,0.25,0.5,0.75,0.95,0.99]).T\n",
    "summary['iqr'] = summary['75%'] - summary['25%']\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd89a6a7",
   "metadata": {},
   "source": [
    "## Step 5 — Segment profiling (Province, Branch, PostalCode, Make, Gender)\n",
    "\n",
    "**What we compute:** Aggregates by segment: TotalPremium, TotalClaims, ClaimFrequency, ClaimSeverity, Margin, LossRatio.\n",
    "\n",
    "**Interpretation tips:**\n",
    "- Focus on segments with materially higher LossRatio and sufficient volume (avoid small-n noise).\n",
    "- High LossRatio + high volume → immediate underwriting/pricing action.\n",
    "- High LossRatio + low volume → flag for investigation but treat cautiously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17afc1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_funcs = {TP:'sum', TC:'sum', 'ClaimFrequency':'mean','ClaimSeverity':'mean','Margin':'mean'}\n",
    "for col in ['Branch','Province','PostalCode','Make','Gender']:\n",
    "    if col in df.columns:\n",
    "        print('\\n===', col, '===')\n",
    "        agg = df.groupby(col).agg(agg_funcs)\n",
    "        agg['LossRatio'] = agg[TC] / agg[TP]\n",
    "        display(agg.sort_values('LossRatio', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7406dd",
   "metadata": {},
   "source": [
    "## Step 6 — Outlier handling options (winsorize & log)\n",
    "\n",
    "**Why:** A few catastrophic claims can dominate averages; we provide winsorized and log-transformed columns for modeling experiments. Keep original columns for business interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6483ca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "def winsorize_series(s, lower=0.01, upper=0.99):\n",
    "    lo = s.quantile(lower)\n",
    "    hi = s.quantile(upper)\n",
    "    return s.clip(lower=lo, upper=hi)\n",
    "\n",
    "df_model = df.copy()\n",
    "for col in [TP, TC, 'Margin', 'ClaimSeverity', 'LossRatio']:\n",
    "    if col in df_model.columns:\n",
    "        df_model[f'{col}_w'] = winsorize_series(df_model[col].fillna(0), lower=0.01, upper=0.99)\n",
    "\n",
    "for col in [TP, TC, 'Margin']:\n",
    "    if col in df_model.columns:\n",
    "        df_model[f'log_{col}'] = np.log1p(df_model[col].clip(lower=0))\n",
    "\n",
    "display(df_model[[TP, f'{TP}_w', f'log_{TP}']].describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a244588",
   "metadata": {},
   "source": [
    "## Step 7 — A/B test function\n",
    "\n",
    "**Tests performed:**\n",
    "- ClaimFrequency: Chi-squared (categorical). If expected counts are small, results may be unreliable; consider combining categories.\n",
    "- ClaimSeverity: Mann–Whitney U by default (non-parametric; robust to heavy tails).\n",
    "- Margin: Mann–Whitney U by default.\n",
    "\n",
    "**Interpretation guidance:**\n",
    "- p < 0.05 → statistically significant difference. Business action should consider effect size and volume (n) before changing pricing.\n",
    "- For many pairwise tests (provinces/postcodes), adjust for multiple testing (Benjamini-Hochberg) before hard decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afe907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abtest(df, col, A, B, alpha=0.05):\n",
    "    out = {'column': col, 'A': A, 'B': B, 'n_A': int((df[col]==A).sum()), 'n_B': int((df[col]==B).sum())}\n",
    "    A_df = df[df[col]==A]\n",
    "    B_df = df[df[col]==B]\n",
    "    # Claim Frequency\n",
    "    try:\n",
    "        table = pd.crosstab(df[col], df['ClaimFrequency'])\n",
    "        if A in table.index and B in table.index:\n",
    "            chi2, p_freq, _, _ = stats.chi2_contingency(table.loc[[A,B]].values)\n",
    "        else:\n",
    "            p_freq = np.nan\n",
    "    except Exception:\n",
    "        p_freq = np.nan\n",
    "    out['p_freq'] = p_freq\n",
    "    # Claim Severity\n",
    "    sevA = A_df['ClaimSeverity'].dropna()\n",
    "    sevB = B_df['ClaimSeverity'].dropna()\n",
    "    out['n_sev_A'] = len(sevA)\n",
    "    out['n_sev_B'] = len(sevB)\n",
    "    if len(sevA)>0 and len(sevB)>0:\n",
    "        try:\n",
    "            p_sev = stats.mannwhitneyu(sevA, sevB, alternative='two-sided').pvalue\n",
    "        except Exception:\n",
    "            p_sev = np.nan\n",
    "    else:\n",
    "        p_sev = np.nan\n",
    "    out['p_severity'] = p_sev\n",
    "    # Margin\n",
    "    mA = A_df['Margin'].dropna()\n",
    "    mB = B_df['Margin'].dropna()\n",
    "    out['n_m_A'] = len(mA)\n",
    "    out['n_m_B'] = len(mB)\n",
    "    if len(mA)>0 and len(mB)>0:\n",
    "        try:\n",
    "            p_m = stats.mannwhitneyu(mA, mB, alternative='two-sided').pvalue\n",
    "        except Exception:\n",
    "            p_m = np.nan\n",
    "    else:\n",
    "        p_m = np.nan\n",
    "    out['p_margin'] = p_m\n",
    "    # Effect sizes (mean differences)\n",
    "    out['mean_sev_A'] = float(sevA.mean()) if len(sevA)>0 else np.nan\n",
    "    out['mean_sev_B'] = float(sevB.mean()) if len(sevB)>0 else np.nan\n",
    "    out['mean_m_A'] = float(mA.mean()) if len(mA)>0 else np.nan\n",
    "    out['mean_m_B'] = float(mB.mean()) if len(mB)>0 else np.nan\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837a054e",
   "metadata": {},
   "source": [
    "## Step 8 — Run A/B tests across key segments\n",
    "\n",
    "**Segments tested:** Branch (if present), Province, Gender, top PostalCodes (by volume), top Makes (by volume).\n",
    "\n",
    "**Interpretation:**\n",
    "Each row in the results should be assessed for: statistical significance (p), effect magnitude (difference in means), and sample sizes (n). Small p but tiny effect size and low n should not drive immediate pricing changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78562eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "def run_pairwise_for(col, top_n=None):\n",
    "    if col not in df.columns:\n",
    "        return\n",
    "    vals = df[col].dropna().value_counts()\n",
    "    if len(vals) < 2:\n",
    "        return\n",
    "    baseline = vals.index[0]  # largest group\n",
    "    if top_n and len(vals) > top_n:\n",
    "        top = vals.nlargest(top_n).index.tolist()\n",
    "        df['_grp'] = df[col].where(df[col].isin(top), 'OTHER')\n",
    "        for v in top:\n",
    "            results.append(abtest(df, '_grp', v, 'OTHER'))\n",
    "    else:\n",
    "        for v in vals.index[1:]:\n",
    "            results.append(abtest(df, col, baseline, v))\n",
    "\n",
    "for c in ['Branch','Province','Gender']:\n",
    "    run_pairwise_for(c)\n",
    "\n",
    "run_pairwise_for('PostalCode', top_n=10)\n",
    "run_pairwise_for('Make', top_n=20)\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "if not res_df.empty:\n",
    "    for c in ['p_freq','p_severity','p_margin']:\n",
    "        res_df[c+'_sig'] = res_df[c].apply(lambda p: 'sig' if (pd.notna(p) and p<0.05) else ('ns' if pd.notna(p) else pd.NA))\n",
    "\n",
    "display(res_df.head(60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69f33c7",
   "metadata": {},
   "source": [
    "## Step 9 — Significant findings (executive interpretation)\n",
    "\n",
    "**How to read the table:**\n",
    "- `p_freq`, `p_severity`, `p_margin`: p-values for the statistical tests\n",
    "- `_sig` suffix indicates whether the test is statistically significant at α=0.05\n",
    "- `mean_*` columns show group means to assess effect direction and magnitude\n",
    "\n",
    "**Business interpretation rules:**\n",
    "1. If Claim Frequency is significant and the higher-frequency group has sufficient volume → consider changes to underwriting rules, targeted risk-mitigation campaigns, or localized marketing.\n",
    "2. If Claim Severity is significant (large positive difference) → investigate claim causes, consider conditional limits or product changes.\n",
    "3. If Margin is significant → prioritize pricing/offer adjustments for profitability.\n",
    "\n",
    "**Caveats:**\n",
    "- Multiple comparisons: many pairwise tests increase false discovery rate. Apply BH correction before final decisions.\n",
    "- Regulatory constraints: check whether demographic pricing (e.g., Gender) is permitted in your jurisdiction before action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f32e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_any = res_df[(res_df['p_freq']<0.05) | (res_df['p_severity']<0.05) | (res_df['p_margin']<0.05)].copy()\n",
    "def rec(row):\n",
    "    notes = []\n",
    "    if row.get('p_freq',1.0) < 0.05:\n",
    "        notes.append('Investigate frequency drivers; consider underwriting/targeting')\n",
    "    if row.get('p_severity',1.0) < 0.05:\n",
    "        notes.append('Investigate severity drivers; consider conditional limits')\n",
    "    if row.get('p_margin',1.0) < 0.05:\n",
    "        notes.append('Review pricing; consider targeted offers or loadings')\n",
    "    return '; '.join(notes) if notes else 'No immediate action'\n",
    "\n",
    "if not sig_any.empty:\n",
    "    sig_any['recommendation'] = sig_any.apply(rec, axis=1)\n",
    "    outp = 'AB_Test_Significant_Findings.csv'\n",
    "    sig_any.to_csv(outp, index=False)\n",
    "    print('Saved:', outp)\n",
    "    display(sig_any.head(50))\n",
    "else:\n",
    "    print('No significant differences found at alpha=0.05')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc49e553",
   "metadata": {},
   "source": [
    "## Step 10 — Visual snapshots for top significant rows\n",
    "\n",
    "For each top significant finding we plot Claim Frequency, Claim Severity and Margin. Use these visuals to present to stakeholders and validate the numeric tests visually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4d3099",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not sig_any.empty:\n",
    "    for idx, row in sig_any.head(6).iterrows():\n",
    "        col = row['column']\n",
    "        A = row['A']; B = row['B']\n",
    "        print(f\"\\n--- Snapshot: {col} | {A} vs {B} ---\")\n",
    "        if col == '_grp':\n",
    "            gcol = '_grp'\n",
    "        else:\n",
    "            gcol = col\n",
    "        tmp = df[df[gcol].isin([A,B])]\n",
    "        plt.figure(figsize=(12,3))\n",
    "        plt.subplot(1,3,1)\n",
    "        sns.barplot(x=gcol, y='ClaimFrequency', data=tmp)\n",
    "        plt.title('Claim Frequency')\n",
    "        plt.subplot(1,3,2)\n",
    "        sns.boxplot(x=gcol, y='ClaimSeverity', data=tmp)\n",
    "        plt.title('Claim Severity')\n",
    "        plt.subplot(1,3,3)\n",
    "        sns.boxplot(x=gcol, y='Margin', data=tmp)\n",
    "        plt.title('Margin')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print('No significant findings to plot.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2a4314",
   "metadata": {},
   "source": [
    "## Final notes & recommended next steps\n",
    "\n",
    "- **Multiple testing correction:** Apply Benjamini-Hochberg (FDR) or Bonferroni when presenting many pairwise results.\n",
    "- **Modeling next steps:** Build frequency (logistic/GBM) and severity (GLM Gamma/XGBoost on claims>0) models. Combine into a pricing formula: Premium ≈ P(claim)*E[severity|claim]*loading.\n",
    "- **Operationalize:** Use DVC for data/versioning and GitHub Actions for CI/CD of experiments (Task 2).\n",
    "- **Regulatory:** Confirm whether demographic/branch-based pricing is permissible.\n",
    "\n",
    "---\n",
    "\n",
    "End of notebook. Run cells sequentially.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
